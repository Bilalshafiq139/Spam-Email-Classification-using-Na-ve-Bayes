# -*- coding: utf-8 -*-
"""Spam email classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p_1ZZHRO1Ov2Koyjh_FASHna70TAnJ3p
"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/spam.csv', encoding='latin-1')

df

df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1)

df

df.sample(5)

df.rename(columns={"v1":"target","v2":"text"},inplace=True)
df.sample(5)

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()

# Correctly encode the target column
df['target'] = encoder.fit_transform(df['target'])

# Display the first few rows
df.head()

# df['target']=encoder.fit(df['target'])

df

df.isnull().sum()

df.duplicated().sum()

df=df.drop_duplicates(keep='first')

df.duplicated().sum()

df.shape

"""**Exploratory data analysis**"""

df["target"].value_counts()

import matplotlib.pyplot as plt
plt.pie(df["target"].value_counts(),labels=["ham","spam"],autopct="%0.2f")
plt.show()

import nltk

!pip install nltk

nltk.download("punkt")

import nltk
nltk.download('punkt')  # Download tokenizer data

df = df.copy()  # Breaks any links to the original DataFrame
df["numer_of_char"] = df["text"].apply(len)

df

import nltk
 nltk.download('punkt_tab')

df["num_words"]=df["text"].apply(lambda x:len((nltk.word_tokenize(x))))

df

df["num_sent"]=df["text"].apply(lambda x:len((nltk.sent_tokenize(x))))

df.head()

df[["target",	"text",	"numer_of_char",	"num_words",	"num_sent"]].describe()

df[df["target"]==0][["numer_of_char","num_words","num_sent"]].describe()

df[df["target"]==1][["numer_of_char","num_words","num_sent"]].describe()

import seaborn as sns

sns.histplot(df[df["target"]==0]["numer_of_char"])#spam msgs
sns.histplot(df[df["target"]==1]["numer_of_char"],color="green")#ham msgs

sns.pairplot(df,hue="target")

sns.heatmap(df[["target",	"numer_of_char",	"num_words",	"num_sent"]].corr(),annot=True)

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
import string

# Download stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')

ps = PorterStemmer()  # Initialize the Porter Stemmer

def transform_text(text):
  text= text.lower()
  text=nltk.word_tokenize(text)


  y=[]
  for i in text:
    if i.isalnum():
      y.append(i)

  text=y[:]
  y.clear()


  for i in text:
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)
  return y

  text=y[:]
  y.clear()

  for i in text:
    y.append(ps.stem(i))

  return " ".join(y)

df["trasformed_text"]=df["text"].apply(transform_text)

df.head()

df["trasformed_text"] = df["trasformed_text"].astype(str)

df.head()

from wordcloud import WordCloud

wc = WordCloud(width=500, height=500, min_font_size=10, background_color="white")

spam_wc=wc.generate(df[df["target"]==1]["trasformed_text"].str.cat(sep=" "))

print(df.dtypes)

plt.imshow(spam_wc)

ham_wc=wc.generate(df[df["target"]==0]["trasformed_text"].str.cat(sep=" "))
plt.imshow(ham_wc)

spam_corpus=[]
for msg in df[df["target"]==1]["trasformed_text"].tolist():
  print(msg)
  for word in msg:
    spam_corpus.append(word)

len(spam_corpus)

from collections import Counter  # Importing Counter

spam_word_counts = Counter(spam_corpus).most_common(30)

df_spam_words = pd.DataFrame(spam_word_counts, columns=["word", "count"])

plt.figure(figsize=(12,6))
sns.barplot(x="word", y="count", data=df_spam_words)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)
plt.show()

ham_corpus=[]
for msg in df[df["target"]==0]["trasformed_text"].tolist():
  print(msg)
  for word in msg:
    ham_corpus.append(word)

len(ham_corpus)

ham_word_counts = Counter(ham_corpus).most_common(30)

df_ham_words = pd.DataFrame(ham_word_counts, columns=["word", "count"])

plt.figure(figsize=(12,6))
sns.barplot(x="word", y="count", data=df_ham_words)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)
plt.show()

"""**Model Buliding**"""

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()

print(type(df["trasformed_text"].iloc[0]))  # Check the data type of the first row

import ast

df["trasformed_text"] = df["trasformed_text"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

df["trasformed_text"] = df["trasformed_text"].apply(lambda x: " ".join(x) if isinstance(x, list) else str(x))

print(type(df["trasformed_text"].iloc[0]))  # Should now be <class 'str'>
print(df["trasformed_text"].head())  # Should print proper sentences, not lists

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
x = cv.fit_transform(df["trasformed_text"]).toarray()
print(x.shape)  # Check dimensions

y=df["target"].values

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

gnb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Ensure X_train and y_train are NumPy arrays
print(type(X_train), type(y_train))  # Should both be <class 'numpy.ndarray'>

# Train model
gnb.fit(X_train, y_train)

# Predictions
y_pred1 = gnb.predict(X_test)

# Evaluate

print(accuracy_score(y_test, y_pred1))
print(confusion_matrix(y_test, y_pred1))
print(precision_score(y_test, y_pred1))

mnb.fit(X_train, y_train)
y_pred2 = gnb.predict(X_test)
print(accuracy_score(y_test, y_pred2))
print(confusion_matrix(y_test, y_pred2))
print(precision_score(y_test, y_pred2))

bnb.fit(X_train, y_train)
y_pred3 = gnb.predict(X_test)
print(accuracy_score(y_test, y_pred3))
print(confusion_matrix(y_test, y_pred3))
print(precision_score(y_test, y_pred3))

